{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>Cuts Optimization using Extra Gradient Boosting\n",
    "<br></p><br>\n",
    "\n",
    "Over the last years, **Machine Learning** tools have been successfully applied to problems in high-energy physics. For example, for the classification of physics objects. Supervised machine learning algorithms allow for significant improvements in classification problems by taking into account observable correlations and by learning the optimal selection from examples, e.g. from Monte Carlo simulations.\n",
    "\n",
    "\n",
    "# Importing the Libraries\n",
    "\n",
    "**Numpy** is a powerful library that makes working with python more efficient, so we will import it and use it as np in the code. **Pandas** is another useful library that is built on numpy and has two great objects *series* and *dataframework*. Pandas works great for *data ingestion* and also has *data visualization* features. From **Hipe4ml** we import **TreeHandler** and with the help of this function we will import our *Analysis Tree* to our notebook.\n",
    "\n",
    "**Matplotlib** comes handy in plotting data while the machine learning is performed by **XGBOOST**. We will import data splitter from **Scikit-learn** as *train_test_split*. **Evaluation metrics** such as *confusion matrix*, *Receiver operating characteristic (ROC)*, and *Area Under the Receiver Operating Characteristic Curve (ROC AUC)*  will be used to asses our models.\n",
    "\n",
    "A **Confusion Matrix** $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$. Thus in binary classification, the count of true positives is $C_{00}$, false negatives $C_{01}$,false positives is $C_{10}$, and true neagtives is $C_{11}$.\n",
    "\n",
    "If $ y^{'}_{i} $ is the predicted value of the $ i$-th sample and $y_{i}$ is the corresponding true value, then the fraction of correct predictions over $ n_{samples}$ is defined as \n",
    "$$\n",
    "True \\: positives (y,y^{'}) =  \\sum_{i=1}^{n_{samples} } 1 (y^{'}_{i} = y_{i}=1)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.22/07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from numpy import sqrt, log\n",
    "from numpy import argmax\n",
    "import weakref \n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import uproot\n",
    "from root_pandas import read_root\n",
    "\n",
    "\n",
    "from data_cleaning import clean_df\n",
    "from KFPF_lambda_cuts import KFPF_lambda_cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save some memory we will delete unused variables\n",
    "class TestClass(object): \n",
    "    def check(self): \n",
    "        print (\"object is alive!\") \n",
    "    def __del__(self): \n",
    "        print (\"object deleted\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "CBM has a modified version of the cern's root software and it contains the simulated setup of CBM. Normally, a model generated input file, for example a URQMD 12 AGeV, is passed through different macros. These macros represent the CBM setup and it is like taking particles and passing them through a detector. These particles are registered as hits in the setup. Then particles' tracks are reconstructed from these hits using cellular automaton and Kalman Filter mathematics.\n",
    "\n",
    "\n",
    "CBM uses the **tree** format of cern root to store information. To reduce the size of these root files a modified tree file was created by the name of Analysis tree. This Analysis tree file contains most of the information that we need for physics analysis. \n",
    "\n",
    "In this example, we download three Analysis Trees. The first one contains mostly background candidates for lambda i.e. protons and pions which do not come from a lambda. The second file contains mostly signal candidates of lamba i.e. it contains protons and pions which come from a lambda decay. The third one contains 10k events generated using URQMD generator with 12 AGeV energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import three root files into our jupyter notebook\n",
    "signal = read_root('/home/shahid/cbmsoft/Data/PFSimplePlainTreeSignal.root','PlainTree')\n",
    "# We only select lambda candidates\n",
    "sgnal = signal[(signal['LambdaCandidates_is_signal']==1) & (signal['LambdaCandidates_mass']>1.108)\n",
    "               & (signal['LambdaCandidates_mass']<1.1227)]\n",
    "\n",
    "# Similarly for the background\n",
    "background = read_root('/home/shahid/cbmsoft/Data/PFSimplePlainTreeBackground.root','PlainTree')\n",
    "bg = background[(background['LambdaCandidates_is_signal'] == 0)\n",
    "                & ((background['LambdaCandidates_mass'] > 1.07)\n",
    "                & (background['LambdaCandidates_mass'] < 1.108) | (background['LambdaCandidates_mass']>1.1227) \n",
    "                   & (background['LambdaCandidates_mass'] < 2.00))]\n",
    "\n",
    "\n",
    "del signal\n",
    "del background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 10k events data set\n",
    "df_original = read_root('/home/shahid/cbmsoft/Data/10k_events_PFSimplePlainTree.root','PlainTree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The labels of the columns in the df data frame are having the prefix LambdaCandidates_ so we rename them\n",
    "new_labels= ['chi2geo', 'chi2primneg', 'chi2primpos', 'chi2topo', 'cosineneg',\n",
    "       'cosinepos', 'cosinetopo', 'distance', 'eta', 'l', 'ldl',\n",
    "       'mass', 'p', 'pT', 'phi', 'px', 'py', 'pz', 'rapidity',\n",
    "             'x', 'y', 'z', 'daughter1id', 'daughter2id', 'isfrompv', 'pid', 'issignal']\n",
    "\n",
    "df_original.columns=new_labels\n",
    "sgnal.columns = new_labels\n",
    "bg.columns = new_labels\n",
    "\n",
    "#Let's see how the dataframe object df looks like\n",
    "#df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data frame object has some columns/features and for them at the very last column the true Monte Carlos information is available. This MC information tells us whether this reconstructed particle was originally produced as a decaying particle or not. So a value of 1 means that it is a true candidate and 0 means that it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Sometimes a data set contains entries which do not make sense. For example, infinite values or NaN entries. We clean the data by removing these entries. Ofcourse, we lose some data points but these outliers sometimes cause problems when we perform analysis. \n",
    "\n",
    "Since our experiment is a fixed target experiment so there are certain constraints which have to be applied on the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new data frame and saving the results in it after cleaning of the original dfs\n",
    "#Also keeping the original one\n",
    "bcknd = clean_df(bg)\n",
    "signal = clean_df(sgnal)\n",
    "df_clean = clean_df(df_original)\n",
    "\n",
    "del bg\n",
    "del sgnal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Background and Signal\n",
    "Our sample contains a lot of background (2178718) and somewhat signal candidates (36203). For analysis we will use a signal set of 4000 candidates and a background set of 12000 candidates. The background and signal candidates will be selected by using MC information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We randomly choose our signal set of 4000 candidates\n",
    "signal_selected= signal.sample(n=90000)\n",
    "\n",
    "#background = 3 times the signal is also done randomly\n",
    "background_selected = bcknd\n",
    "\n",
    "del signal\n",
    "del bcknd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's combine signal and background\n",
    "dfs = [signal_selected, background_selected]\n",
    "df_scaled = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chi2geo</th>\n",
       "      <th>chi2primneg</th>\n",
       "      <th>chi2primpos</th>\n",
       "      <th>chi2topo</th>\n",
       "      <th>cosineneg</th>\n",
       "      <th>cosinepos</th>\n",
       "      <th>cosinetopo</th>\n",
       "      <th>distance</th>\n",
       "      <th>eta</th>\n",
       "      <th>l</th>\n",
       "      <th>...</th>\n",
       "      <th>pz</th>\n",
       "      <th>rapidity</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>daughter1id</th>\n",
       "      <th>daughter2id</th>\n",
       "      <th>isfrompv</th>\n",
       "      <th>pid</th>\n",
       "      <th>issignal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125456</th>\n",
       "      <td>1.264959</td>\n",
       "      <td>3603.395508</td>\n",
       "      <td>2978.606445</td>\n",
       "      <td>1.204196</td>\n",
       "      <td>0.940794</td>\n",
       "      <td>0.998405</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.181247</td>\n",
       "      <td>2.420511</td>\n",
       "      <td>8.167913</td>\n",
       "      <td>...</td>\n",
       "      <td>1.984055</td>\n",
       "      <td>1.298298</td>\n",
       "      <td>1.546101</td>\n",
       "      <td>-0.058854</td>\n",
       "      <td>8.027164</td>\n",
       "      <td>471.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189803</th>\n",
       "      <td>1.547112</td>\n",
       "      <td>15.087948</td>\n",
       "      <td>0.095412</td>\n",
       "      <td>2.714692</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.230369</td>\n",
       "      <td>3.480420</td>\n",
       "      <td>4.222751</td>\n",
       "      <td>...</td>\n",
       "      <td>4.591040</td>\n",
       "      <td>2.068335</td>\n",
       "      <td>0.048561</td>\n",
       "      <td>-0.333145</td>\n",
       "      <td>4.222316</td>\n",
       "      <td>240.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477015</th>\n",
       "      <td>6.923366</td>\n",
       "      <td>8.003510</td>\n",
       "      <td>6.882685</td>\n",
       "      <td>5.426916</td>\n",
       "      <td>0.990920</td>\n",
       "      <td>0.992231</td>\n",
       "      <td>0.983067</td>\n",
       "      <td>0.124541</td>\n",
       "      <td>2.454316</td>\n",
       "      <td>0.100614</td>\n",
       "      <td>...</td>\n",
       "      <td>2.329260</td>\n",
       "      <td>1.291904</td>\n",
       "      <td>0.192930</td>\n",
       "      <td>-0.141938</td>\n",
       "      <td>0.086587</td>\n",
       "      <td>127.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57654</th>\n",
       "      <td>5.728300</td>\n",
       "      <td>2.833282</td>\n",
       "      <td>3.312725</td>\n",
       "      <td>2.047969</td>\n",
       "      <td>0.997508</td>\n",
       "      <td>0.997320</td>\n",
       "      <td>0.646019</td>\n",
       "      <td>0.005759</td>\n",
       "      <td>2.224169</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>...</td>\n",
       "      <td>5.049699</td>\n",
       "      <td>1.766251</td>\n",
       "      <td>-0.015536</td>\n",
       "      <td>0.018110</td>\n",
       "      <td>0.006361</td>\n",
       "      <td>126.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330663</th>\n",
       "      <td>0.868387</td>\n",
       "      <td>543231.812500</td>\n",
       "      <td>35190.375000</td>\n",
       "      <td>0.470812</td>\n",
       "      <td>0.944284</td>\n",
       "      <td>0.998617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>2.901293</td>\n",
       "      <td>5.722093</td>\n",
       "      <td>...</td>\n",
       "      <td>2.141836</td>\n",
       "      <td>1.387511</td>\n",
       "      <td>-0.105896</td>\n",
       "      <td>0.871515</td>\n",
       "      <td>5.693000</td>\n",
       "      <td>93.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556000</th>\n",
       "      <td>5.717398</td>\n",
       "      <td>10.896736</td>\n",
       "      <td>1.593740</td>\n",
       "      <td>3.830730</td>\n",
       "      <td>0.957055</td>\n",
       "      <td>0.994079</td>\n",
       "      <td>0.990417</td>\n",
       "      <td>0.053834</td>\n",
       "      <td>2.623692</td>\n",
       "      <td>0.096179</td>\n",
       "      <td>...</td>\n",
       "      <td>2.606829</td>\n",
       "      <td>1.458732</td>\n",
       "      <td>0.021958</td>\n",
       "      <td>-0.093280</td>\n",
       "      <td>0.093925</td>\n",
       "      <td>67.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87070</th>\n",
       "      <td>1.602541</td>\n",
       "      <td>2706.367188</td>\n",
       "      <td>77.682251</td>\n",
       "      <td>0.908254</td>\n",
       "      <td>0.996489</td>\n",
       "      <td>0.999876</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.150611</td>\n",
       "      <td>2.805558</td>\n",
       "      <td>9.678487</td>\n",
       "      <td>...</td>\n",
       "      <td>7.685510</td>\n",
       "      <td>2.366184</td>\n",
       "      <td>1.145822</td>\n",
       "      <td>0.166275</td>\n",
       "      <td>9.612197</td>\n",
       "      <td>208.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328496</th>\n",
       "      <td>0.584094</td>\n",
       "      <td>9.402885</td>\n",
       "      <td>0.518631</td>\n",
       "      <td>2.899597</td>\n",
       "      <td>0.876312</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>0.999769</td>\n",
       "      <td>0.180235</td>\n",
       "      <td>2.796551</td>\n",
       "      <td>1.617604</td>\n",
       "      <td>...</td>\n",
       "      <td>6.828983</td>\n",
       "      <td>2.153287</td>\n",
       "      <td>0.083520</td>\n",
       "      <td>-0.081467</td>\n",
       "      <td>1.605318</td>\n",
       "      <td>105.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228027</th>\n",
       "      <td>3.592514</td>\n",
       "      <td>8.207214</td>\n",
       "      <td>1.979630</td>\n",
       "      <td>2.162431</td>\n",
       "      <td>0.913905</td>\n",
       "      <td>0.992617</td>\n",
       "      <td>0.993367</td>\n",
       "      <td>0.155657</td>\n",
       "      <td>3.498562</td>\n",
       "      <td>0.295184</td>\n",
       "      <td>...</td>\n",
       "      <td>3.857150</td>\n",
       "      <td>1.713483</td>\n",
       "      <td>-0.000852</td>\n",
       "      <td>0.188907</td>\n",
       "      <td>0.284930</td>\n",
       "      <td>321.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529249</th>\n",
       "      <td>19.583164</td>\n",
       "      <td>6.007696</td>\n",
       "      <td>340288.937500</td>\n",
       "      <td>4457.123047</td>\n",
       "      <td>0.960249</td>\n",
       "      <td>0.990474</td>\n",
       "      <td>0.957127</td>\n",
       "      <td>0.501260</td>\n",
       "      <td>3.361045</td>\n",
       "      <td>16.679560</td>\n",
       "      <td>...</td>\n",
       "      <td>2.098746</td>\n",
       "      <td>1.301835</td>\n",
       "      <td>1.276144</td>\n",
       "      <td>3.741682</td>\n",
       "      <td>16.985256</td>\n",
       "      <td>106.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          chi2geo    chi2primneg    chi2primpos     chi2topo  cosineneg  \\\n",
       "125456   1.264959    3603.395508    2978.606445     1.204196   0.940794   \n",
       "189803   1.547112      15.087948       0.095412     2.714692   0.999656   \n",
       "477015   6.923366       8.003510       6.882685     5.426916   0.990920   \n",
       "57654    5.728300       2.833282       3.312725     2.047969   0.997508   \n",
       "330663   0.868387  543231.812500   35190.375000     0.470812   0.944284   \n",
       "556000   5.717398      10.896736       1.593740     3.830730   0.957055   \n",
       "87070    1.602541    2706.367188      77.682251     0.908254   0.996489   \n",
       "328496   0.584094       9.402885       0.518631     2.899597   0.876312   \n",
       "228027   3.592514       8.207214       1.979630     2.162431   0.913905   \n",
       "529249  19.583164       6.007696  340288.937500  4457.123047   0.960249   \n",
       "\n",
       "        cosinepos  cosinetopo  distance       eta          l  ...        pz  \\\n",
       "125456   0.998405    0.999971  0.181247  2.420511   8.167913  ...  1.984055   \n",
       "189803   0.999940    0.999941  0.230369  3.480420   4.222751  ...  4.591040   \n",
       "477015   0.992231    0.983067  0.124541  2.454316   0.100614  ...  2.329260   \n",
       "57654    0.997320    0.646019  0.005759  2.224169   0.000694  ...  5.049699   \n",
       "330663   0.998617    1.000000  0.004988  2.901293   5.722093  ...  2.141836   \n",
       "556000   0.994079    0.990417  0.053834  2.623692   0.096179  ...  2.606829   \n",
       "87070    0.999876    0.999996  0.150611  2.805558   9.678487  ...  7.685510   \n",
       "328496   0.999768    0.999769  0.180235  2.796551   1.617604  ...  6.828983   \n",
       "228027   0.992617    0.993367  0.155657  3.498562   0.295184  ...  3.857150   \n",
       "529249   0.990474    0.957127  0.501260  3.361045  16.679560  ...  2.098746   \n",
       "\n",
       "        rapidity         x         y          z  daughter1id  daughter2id  \\\n",
       "125456  1.298298  1.546101 -0.058854   8.027164        471.0        252.0   \n",
       "189803  2.068335  0.048561 -0.333145   4.222316        240.0        162.0   \n",
       "477015  1.291904  0.192930 -0.141938   0.086587        127.0        109.0   \n",
       "57654   1.766251 -0.015536  0.018110   0.006361        126.0        171.0   \n",
       "330663  1.387511 -0.105896  0.871515   5.693000         93.0         77.0   \n",
       "556000  1.458732  0.021958 -0.093280   0.093925         67.0         62.0   \n",
       "87070   2.366184  1.145822  0.166275   9.612197        208.0        175.0   \n",
       "328496  2.153287  0.083520 -0.081467   1.605318        105.0         71.0   \n",
       "228027  1.713483 -0.000852  0.188907   0.284930        321.0         54.0   \n",
       "529249  1.301835  1.276144  3.741682  16.985256        106.0        416.0   \n",
       "\n",
       "        isfrompv     pid  issignal  \n",
       "125456       0.0  3122.0       1.0  \n",
       "189803       0.0  3122.0       0.0  \n",
       "477015       0.0  3122.0       0.0  \n",
       "57654        0.0  3122.0       0.0  \n",
       "330663       0.0  3122.0       1.0  \n",
       "556000       0.0  3122.0       0.0  \n",
       "87070        0.0  3122.0       1.0  \n",
       "328496       0.0  3122.0       0.0  \n",
       "228027       0.0  3122.0       0.0  \n",
       "529249       0.0  3122.0       0.0  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's shuffle the rows randomly\n",
    "df_scaled = df_scaled.sample(frac=1)\n",
    "del dfs\n",
    "# Let's take a look at the top 10 entries of the df\n",
    "df_scaled.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "range1 = (1.075, 1.18)\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "#df_scaled['mass'].plot.hist(bins = 300, range=range1,grid=True,sharey=True)\n",
    "df_scaled['mass'].plot.hist(bins = 300, facecolor='yellow',grid=True,range=range1)\n",
    "signal_selected['mass'].plot.hist(bins = 300, facecolor='magenta',grid=True, range=range1)\n",
    "plt.ylabel(\"Counts\", fontsize=15)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize= 15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title('Test and Train Lambda Invariant Mass', fontsize = 15)\n",
    "plt.legend(('Background', 'Signal'), fontsize = 15)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"signal_bac_invmass_MC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Test sets\n",
    "To make machine learning algorithms more efficient on unseen data we divide our data into two sets. One set is for training the algorithm and the other is for testing the algorithm. If we don't do this then the algorithm can overfit and we will not capture the general trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'chi2primneg', 'chi2primpos', 'ldl', 'distance', 'chi2geo','cosinepos']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int')\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "x_whole = df_clean[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole = pd.DataFrame(df_clean['issignal'], dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a new df \n",
    "new_check_set=KFPF_lambda_cuts(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>XGB Boost \n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian\n",
    "In order to find the best parameters of XGB for our data we use Bayesian optimization. Grid search and and random search could also do the same job but bayesian is more time efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "dtest1=xgb.DMatrix(x_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n",
    "\n",
    "*subsample* [default=1]\n",
    "Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "range: (0,1]\n",
    "\n",
    "*eta* [default=0.3, alias: learning_rate]\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "range: [0,1]\n",
    "\n",
    "\n",
    "*gamma* [default=0, alias: min_split_loss]\n",
    "Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "range: [0,âˆž]\n",
    "\n",
    "\n",
    "*alpha* [default=0, alias: reg_alpha]\n",
    "L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "*Lasso Regression* (Least Absolute Shrinkage and Selection Operator) adds â€œabsolute value of magnitudeâ€ of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Optimization function for xgboost\n",
    "#specify the parameters you want to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.1,\n",
    "              'eval_metric': 'auc', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=70, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0,1),\n",
    "                                             'n_estimators':(100,500)\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=15, init_points=8, acq='ei')\n",
    "#0.9952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best target so far 0.994872\n",
    "print(xgb_bo.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New models for treelite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'], 'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), 'objective': 'binary:logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst1= bst.predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_test = bst.predict(dtest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AMS(y_true, y_predict, y_true1, y_predict1):\n",
    "    roc_auc=roc_auc_score(y_true, y_predict)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    S0 = S0[~np.isnan(S0)]\n",
    "    xi = argmax(S0)\n",
    "    S0_best_threshold = (thresholds[xi])\n",
    "    \n",
    "    roc_auc1=roc_auc_score(y_true1, y_predict1)\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    S01 = S01[~np.isnan(S01)]\n",
    "    xi1 = argmax(S01)\n",
    "    S0_best_threshold1 = (thresholds[xi1])\n",
    "    \n",
    "    fig, axs = plt.subplots(figsize=(15, 10), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linestyle=':',color='darkorange',label='ROC curve train (area = %0.4f)' % roc_auc)\n",
    "    plt.plot(fpr1, tpr1, color='green',label='ROC curve test (area = %0.4f)' % roc_auc1)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n S0 = '+ \"%.2f\" % S0[xi])\n",
    "    plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n S0 = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 15)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 15)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 15)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 15)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    #axs.axis([-0.01, 1, 0.9, 1])\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMS(y_train, bst1,y_test, bst_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'] = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "df_clean['xgb_preds'].plot.hist(ax=ax, bins=300,facecolor='red',alpha = 0.3, bottom=0.001)\n",
    "TP = df_clean[(df_clean['issignal']==1)]\n",
    "TP['xgb_preds'].plot.hist(ax=ax, bins=300,facecolor='blue',alpha = 0.3, bottom=0.001)\n",
    "ax.set_yscale('log')\n",
    "plt.legend(('XGB predicted probability distribution','True positives = (MC =1) True lamdas in the distribution'), fontsize = 15, loc='upper right')\n",
    "plt.title(\"XGB returned probablility for the 10k events data set\", fontsize = 15)\n",
    "plt.ylabel(\"Log scale number of counts\", fontsize = 15)\n",
    "plt.xlabel(\"Probability\", fontsize = 15)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc=roc_auc_score(y_whole, df_clean['xgb_preds'])\n",
    "fpr, tpr, thresholds = roc_curve(y_whole, df_clean['xgb_preds'],drop_intermediate=False ,pos_label=1)\n",
    "S0 = sqrt(2 * ((tpr + fpr) * log(1 + tpr/fpr) - tpr))\n",
    "S0 = S0[~np.isnan(S0)]\n",
    "xi = argmax(S0)\n",
    "S0_best_threshold = (thresholds[xi])\n",
    "print('Best Threshold=%f, S0=%.3f' % (thresholds[xi], S0[xi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dtest.get_label()\n",
    "print('error=%f' %\n",
    "      (sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i]) /\n",
    "       float(len(preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite\n",
    "model = treelite.Model.from_xgboost(bst)\n",
    "toolchain = 'clang'\n",
    "model.export_lib(toolchain=toolchain, libpath='./mymodel.so',\n",
    "                 params={'parallel_comp': 8}, verbose=True)\n",
    "\n",
    "# Operating system of the target machine\n",
    "platform = 'unix'\n",
    "\n",
    "# Save the source package as a zip archive named mymodel.zip\n",
    "# Later, we'll use this package to produce the library mymodel.so.\n",
    "model.export_srcpkg(platform=platform, toolchain=toolchain,\n",
    "                    pkgpath='./mymodel.zip', libname='mymodel.so',\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -l mymodel.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite_runtime     # runtime module\n",
    "predictor = treelite_runtime.Predictor('./mymodel.so', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_nump = pd.DataFrame.to_numpy(x_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = treelite_runtime.Batch.from_npy2d(x_whole_nump)\n",
    "df_clean['tree_lite'] = predictor.predict(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(out_pred,bins=300, range=(0.5,1) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Already working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(learning_rate = 0.09273,\n",
    "                max_depth = 9, alpha = 3.085, gamma =0.1481 , n_estimators = 115, nthread=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = xg_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xg_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'] =xg_reg.predict(x_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_score = roc_auc_score(y_train, xg_reg.predict(x_train))\n",
    "print(\"AUC: %f\" % (AUC_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_whole, df_clean['xgb_preds']))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_whole, df_clean['xgb_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "ax.figure.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_visualization(cut, range1=(1.09, 1.19), bins1= 300 ):\n",
    "    mask1 = df_clean['xgb_preds']>cut\n",
    "    df3=df_clean[mask1]\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "    color = 'tab:blue'\n",
    "    ax2.hist(df_clean['mass'],bins = bins1, range=range1, facecolor='blue',alpha = 0.35, label='before selection')\n",
    "    ax2.set_ylabel('Counts', fontsize = 15, color=color)\n",
    "    ax2.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend( fontsize = 15, loc='upper left')\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.hist(df3['mass'], bins = bins1, range=range1, facecolor='red',alpha = 0.35, label='Machine learning (XGB)')\n",
    "    ax1.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax1.set_ylabel('Counts ', fontsize = 15, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend( fontsize = 15,loc='upper right' )\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.title(\"The sample's Invariant Mass with XGB (with a cut > \"+str(cut)+')', fontsize = 15)\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_visualization(S0_best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = df_clean['xgb_preds']>0.969618\n",
    "df3=df_clean[mask1]\n",
    "df3_clean_signal_mask = df3['issignal']>0\n",
    "df3_clean_signal = df3[df_clean_signal_mask]\n",
    "df3_clean_bac_mask = df3['issignal']==0\n",
    "df3_clean_bac = df3[df_clean_bac_mask]\n",
    "#vars_to_draw.remove('xgb_preds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask10 = (df_clean['xgb_preds1']==1)&(df_clean['issignal']==1)\n",
    "df4=df_clean[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_labels = ['original background', 'background inside XGB predicted signal']\n",
    "vars_to_draw = list(df3.columns)\n",
    "ax = plot_utils.plot_distr([df_clean_bac, df3_clean_bac], vars_to_draw, bins=100, labels=leg_labels, log=True, density=True, figsize=(30, 20), alpha=0.3, grid=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = S0_best_threshold\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3=df_clean[mask1]\n",
    "fig, axs = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "range1= (1.09, 1.7)\n",
    "\n",
    "#xgb\n",
    "\n",
    "df3_new = df3[df3['issignal']==1]\n",
    "df3_new1 = df3[df3['issignal']==0]\n",
    "df3['mass'].plot.hist(bins = 300, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True)\n",
    "#df3_new['mass'].plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True)\n",
    "df3_new1['mass'].plot.hist(bins = 300, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True)\n",
    "plt.legend(('XGB selected lambdas','\\n False positives = \\n (MC =0)\\n background in \\n the distribution' ), fontsize = 18, loc='upper right')\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "plt.title(\"KFPF variables + cos$\\Theta_{between \\ \\overrightarrow{P_\\Lambda} \\  & \\ \\overrightarrow{P_{\\Pi^-}}}$ + $P_T$  with a cut of %.4f \"%cut3 +\"on the XGB probability distribution\", fontsize = 18)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "By definition a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.\n",
    "\n",
    "Thus in binary classification, the count of true positives is $C_{0,0}$, false positives is $C_{1,0}$, true negatives is $C_{1,1}$ and false negatives is $C_{0,1}$.\n",
    "\n",
    "The following function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize = 15)\n",
    "    plt.xlabel('Predicted label',fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut1 = S0_best_threshold\n",
    "df_clean['xgb_preds1'] = ((df_clean['xgb_preds']>cut1)*1)\n",
    "cnf_matrix = confusion_matrix(y_whole, df_clean['xgb_preds1'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['signal','background'], title='Confusion Matrix for XGB for cut > '+str(cut1))\n",
    "plt.savefig('confusion_matrix_extreme_gradient_boosting_whole_data.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>Tree visualization\n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax1 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "xgb.plot_tree(xg_reg,num_trees=10)\n",
    "plt.rcParams['figure.figsize'] = [20, 40]\n",
    "plt.rcParams['figure.dpi']=200\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.to_graphviz(xg_reg, fmap='', num_trees=0, rankdir=None, yes_color=None, no_color=None, condition_node_params=None, leaf_node_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define objective function\n",
    "def f(x):\n",
    "    return {'loss': x ** 2 - x, 'status': STATUS_OK}\n",
    "\n",
    "# Run hyperopt optimization\n",
    "trials = Trials()\n",
    "result = fmin(\n",
    "    fn=f,                           # objective function\n",
    "    space=hp.uniform('x', -1, 1),   # parameter space\n",
    "    algo=tpe.suggest,               # surrogate algorithm\n",
    "    max_evals=500,                  # no. of evaluations\n",
    "    trials=trials                   # trials object that keeps track of the sample results (optional)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Print the optimized parameters\n",
    "print(result)   # {'x': 0.5000833960783931}\n",
    "\n",
    "# Extract and plot the trials \n",
    "x = trials.vals['x']\n",
    "y = [x['loss'] for x in trials.results]\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# XGB parameters\n",
    "xgb_reg_params = {\n",
    "    'learning_rate':    hp.choice('learning_rate',    np.arange(0.05, 0.31, 0.05)),\n",
    "    'max_depth':        hp.choice('max_depth',        np.arange(5, 16, 1, dtype=int)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n",
    "    'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n",
    "    'subsample':        hp.uniform('subsample', 0.8, 1),\n",
    "    'n_estimators':     100,\n",
    "}\n",
    "xgb_fit_params = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'early_stopping_rounds': 10,\n",
    "    'verbose': False\n",
    "}\n",
    "xgb_para = dict()\n",
    "xgb_para['reg_params'] = xgb_reg_params\n",
    "xgb_para['fit_params'] = xgb_fit_params\n",
    "xgb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
    "\n",
    "\n",
    "class HPOpt(object):\n",
    "\n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.x_test  = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test  = y_test\n",
    "\n",
    "    def process(self, fn_name, space, trials, algo, max_evals):\n",
    "        fn = getattr(self, fn_name)\n",
    "        try:\n",
    "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
    "        except Exception as e:\n",
    "            return {'status': STATUS_FAIL,\n",
    "                    'exception': str(e)}\n",
    "        return result, trials\n",
    "\n",
    "    def xgb_reg(self, para):\n",
    "        reg = xgb.XGBRegressor(**para['reg_params'])\n",
    "        return self.train_reg(reg, para)\n",
    "\n",
    "    def train_reg(self, reg, para):\n",
    "        reg.fit(self.x_train, self.y_train,\n",
    "                eval_set=[(self.x_train, self.y_train), (self.x_test, self.y_test)],\n",
    "                **para['fit_params'])\n",
    "        pred = reg.predict(self.x_test)\n",
    "        loss = para['loss_func'](self.y_test, pred)\n",
    "        return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = HPOpt(x_train, x_test, y_train, y_test)\n",
    "\n",
    "xgb_opt = obj.process(fn_name='xgb_reg', space=xgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clf = xgb.XGBClassifier()\n",
    "model_hdl = ModelHandler(model_clf, cuts)\n",
    "train_test_data = train_test_generator([y, x], [1,0], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_pars_ranges = {'n_estimators': (200, 1000), 'max_depth': (2, 4), 'learning_rate': (0.01, 0.1)}\n",
    "model_hdl.optimize_params_bayes(df_, hyper_pars_ranges, 'roc_auc', nfold=5, init_points=5, n_iter=5, njobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will display the inavriant mass histogram of the original 10k event set along with the mass histoigram after we apply a cut\n",
    "# on the probability prediction of xgb\n",
    "def cut_visualization(cut, range1=(1.09, 1.19), bins1= 300 ):\n",
    "    mask1 = df_clean['xgb_preds']>cut\n",
    "    df3=df_clean[mask1]\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "    color = 'tab:blue'\n",
    "    ax2.hist(new_check_set['mass'],bins = bins1, range=range1, facecolor='blue',alpha = 0.35, label='KFPF')\n",
    "    ax2.set_ylabel('Counts', fontsize = 15, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend( fontsize = 15, loc='upper left')\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.hist(df3['mass'], bins = bins1, range=range1, facecolor='red',alpha = 0.35, label='XGB')\n",
    "    ax1.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax1.set_ylabel('Counts ', fontsize = 15, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend( fontsize = 15,loc='upper right' )\n",
    "\n",
    "    plt.title(\"The original sample's Invariant Mass along with mass after selection of XGB (with a cut > \"+str(cut)+')', fontsize = 15)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_visualization(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_check_set= df_clean.copy()\n",
    "new_check_set['new_signal']=0\n",
    "mask1 = (new_check_set['chi2primpos'] > 18.4) & (new_check_set['chi2primneg'] > 18.4)\n",
    "\n",
    "mask2 = (new_check_set['ldl'] > 5) & (new_check_set['distance'] < 1)\n",
    "\n",
    "mask3 = (new_check_set['chi2geo'] < 3) & (new_check_set['cosinepos'] > 0) & (new_check_set['cosineneg'] > 0)\n",
    "\n",
    "new_check_set = new_check_set[(mask1) & (mask2) & (mask3)] \n",
    "\n",
    "#After all these cuts, what is left is considered as signal, so we replace all the values in the 'new_signal'\n",
    "# column by 1\n",
    "new_check_set['new_signal'] = 1\n",
    "new_check_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = S0_best_threshold\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3=df_clean[mask1]\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "range1= (1.09, 1.17)\n",
    "\n",
    "#xgb\n",
    "\n",
    "\n",
    "(df3['mass']).plot.hist(bins = 300, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True)\n",
    "(new_check_set['mass']).plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True)\n",
    "plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "plt.ylabel(\"counts\", fontsize = 15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(('XGBoost','KFPF'), fontsize = 15, loc='upper left')\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "plt.title(\"The lambda's Invariant Mass histogram with KFPF and XGB selection criteria\", fontsize = 15)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (1.09, 1.17)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3_base['mass']),bins = 300, range=range1, facecolor='red',alpha = 0.3)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 300, range=range1,facecolor='blue',alpha = 0.3)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 15)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost Selected $\\Lambda$s','KFPF selected $\\Lambda$s'), fontsize = 15, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The lambda's Invariant Mass histogram with KFPF and XGB selection criteria on KFPF variables\", fontsize = 15)\n",
    "axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3_base['mass'],range=(1.09, 1.17), bins=300)\n",
    "hist2, bin_edges2 = np.histogram(new_check_set['mass'],range=(1.09, 1.17), bins=300)\n",
    "\n",
    "#makes sense to have only positive values \n",
    "diff = (hist1 - hist2)\n",
    "axs[1].bar(bins[:-1],     # this is what makes it comparable\n",
    "        ns / ns1, # maybe check for div-by-zero!\n",
    "        width=0.001)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 15)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 15)\n",
    "axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = S0_best_threshold\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3_base_cospos=df_clean[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = S0_best_threshold\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3_base=df_clean[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (1.09, 1.17)\n",
    "bin1 = 200\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3_base_cospos['mass']),bins = bin1, range=range1, facecolor='red',alpha = 0.3)\n",
    "ns1, bins1, patches1=axs[0].hist((df3_base['mass']),bins = bin1, range=range1,facecolor='blue',alpha = 0.3)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 15)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGB base+ cos$\\Theta_{between \\ \\overrightarrow{P_\\Lambda} \\  & \\ \\overrightarrow{P_{proton}}}$ Selected $\\Lambda$s','XGB base selected $\\Lambda$s'), fontsize = 15, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The lambda's Invariant Mass histogram with XGB base and XGB base+ cos$\\Theta_{between \\ \\overrightarrow{P_\\Lambda} \\  & \\ \\overrightarrow{P_{proton}}}$ selection criteria\", fontsize = 15)\n",
    "axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3_base_cospos['mass'],range=range1, bins=bin1)\n",
    "hist2, bin_edges2 = np.histogram(df3_base['mass'],range=range1, bins=bin1)\n",
    "\n",
    "#makes sense to have only positive values \n",
    "diff = (hist1 - hist2)\n",
    "axs[1].bar(bins[:-1],     # this is what makes it comparable\n",
    "        ns / ns1, # maybe check for div-by-zero!\n",
    "        width=0.001)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 15)\n",
    "axs[1].set_ylabel(\"XGB base+cospos / XGB base\", fontsize = 15)\n",
    "axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
