{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>Cuts Optimization using Extra Gradient Boosting\n",
    "<br></p><br>\n",
    "\n",
    "Over the last years, **Machine Learning** tools have been successfully applied to problems in high-energy physics. For example, for the classification of physics objects. Supervised machine learning algorithms allow for significant improvements in classification problems by taking into account observable correlations and by learning the optimal selection from examples, e.g. from Monte Carlo simulations.\n",
    "\n",
    "\n",
    "# Importing the Libraries\n",
    "\n",
    "**Numpy** is a powerful library that makes working with python more efficient, so we will import it and use it as np in the code. **Pandas** is another useful library that is built on numpy and has two great objects *series* and *dataframework*. Pandas works great for *data ingestion* and also has *data visualization* features. From **Hipe4ml** we import **TreeHandler** and with the help of this function we will import our *Analysis Tree* to our notebook.\n",
    "\n",
    "**Matplotlib** comes handy in plotting data while the machine learning is performed by **XGBOOST**. We will import data splitter from **Scikit-learn** as *train_test_split*. **Evaluation metrics** such as *confusion matrix*, *Receiver operating characteristic (ROC)*, and *Area Under the Receiver Operating Characteristic Curve (ROC AUC)*  will be used to asses our models.\n",
    "\n",
    "A **Confusion Matrix** $C$ is such that $C_{ij}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$. Thus in binary classification, the count of true positives is $C_{00}$, false negatives $C_{01}$,false positives is $C_{10}$, and true neagtives is $C_{11}$.\n",
    "\n",
    "If $ y^{'}_{i} $ is the predicted value of the $ i$-th sample and $y_{i}$ is the corresponding true value, then the fraction of correct predictions over $ n_{samples}$ is defined as \n",
    "$$\n",
    "True \\: positives (y,y^{'}) =  \\sum_{i=1}^{n_{samples} } 1 (y^{'}_{i} = y_{i}=1)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hipe4ml.model_handler import ModelHandler\n",
    "from hipe4ml.tree_handler import TreeHandler\n",
    "from hipe4ml import plot_utils\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from numpy import sqrt, log\n",
    "from numpy import argmax\n",
    "import weakref \n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import uproot\n",
    "from root_pandas import read_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save some memory we will delete unused variables\n",
    "class TestClass(object): \n",
    "    def check(self): \n",
    "        print (\"object is alive!\") \n",
    "    def __del__(self): \n",
    "        print (\"object deleted\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "executor = ThreadPoolExecutor(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import resource\n",
    "import uproot\n",
    "\n",
    "def peak_memory_usage():\n",
    "    \"\"\"Return peak memory usage in MB\"\"\"\n",
    "    mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "    factor_mb = 1 / 1024\n",
    "    if sys.platform == \"darwin\":\n",
    "        factor_mb = 1 / (1024 * 1024)\n",
    "    return mem * factor_mb\n",
    "\n",
    "input_tree = uproot.open('/home/shahid/cbmsoft/Data/10k_events_PFSimplePlainTree.root:PlainTree')\n",
    "branches = input_tree.arrays()\n",
    "print(peak_memory_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data\n",
    "CBM has a modified version of the cern's root software and it contains the simulated setup of CBM. Normally, a model generated input file, for example a URQMD 12 AGeV, is passed through different macros. These macros represent the CBM setup and it is like taking particles and passing them through a detector. These particles are registered as hits in the setup. Then particles' tracks are reconstructed from these hits using cellular automaton and Kalman Filter mathematics.\n",
    "\n",
    "\n",
    "CBM uses the **tree** format of cern root to store information. To reduce the size of these root files a modified tree file was created by the name of Analysis tree. This Analysis tree file contains most of the information that we need for physics analysis. \n",
    "\n",
    "In this example, we download three Analysis Trees. The first one contains mostly background candidates for lambda i.e. protons and pions which do not come from a lambda. The second file contains mostly signal candidates of lamba i.e. it contains protons and pions which come from a lambda decay. The third one contains 10k events generated using URQMD generator with 12 AGeV energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the tree 'PlainTree' from root file into our jupyter notebook\n",
    "signal = uproot.open('/home/shahid/cbmsoft/Data/PFSimplePlainTreeSignal.root:PlainTree', num_workers=8).arrays(library='pd')\n",
    "# We only select lambda candidates\n",
    "sgnal = signal[(signal['LambdaCandidates_is_signal']==1) & (signal['LambdaCandidates_mass']>1.108) & (signal['LambdaCandidates_mass']<1.1227)\n",
    "\n",
    "del signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original1 = read_root('/home/shahid/cbmsoft/Data/cbm/apr20_fr_18.2.1_fs_jun19p1/urqmd_pluto/auau/12agev/mbias/sis100_electron/TGeant4/analysis_tree/PFSimpleOutput.1.root','sTree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = read_root('/home/shahid/cbmsoft/Data/PFSimplePlainTreeSignal.root','PlainTree')\n",
    "sgnal = signal[(signal['LambdaCandidates_is_signal']==1) & (signal['LambdaCandidates_mass']>1.108)\n",
    "               & (signal['LambdaCandidates_mass']<1.1227)]\n",
    "\n",
    "\n",
    "background = read_root('/home/shahid/cbmsoft/Data/PFSimplePlainTreeBackground.root','PlainTree')\n",
    "bg = background[(background['LambdaCandidates_is_signal'] == 0)\n",
    "                & ((background['LambdaCandidates_mass'] > 1.07)\n",
    "                & (background['LambdaCandidates_mass'] < 1.108) | (background['LambdaCandidates_mass']>1.1227) \n",
    "                   & (background['LambdaCandidates_mass'] < 2.00))]\n",
    "\n",
    "\n",
    "del signal\n",
    "del background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = read_root('/home/shahid/cbmsoft/Data/10k_events_PFSimplePlainTree.root','PlainTree')\n",
    "#df_original = df_original1[(df_original1['LambdaCandidates_is_signal']==1) & (df_original1['LambdaCandidates_mass']>1.108) & (df_original1['LambdaCandidates_mass']<1.1227)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tree = uproot.open('/home/shahid/cbmsoft/Data/10k_events_PFSimplePlainTree.root:PlainTree', decompression_executor=executor, interpretation_executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = input_tree.arrays(library='pd', decompression_executor=executor, interpretation_executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tree_signal = TreeHandler('/home/shahid/cbmsoft/Data/PFSimplePlainTreeSignal.root','PlainTree')\n",
    "sgnal = input_tree_signal.get_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import three root files into our jupyter notebook\n",
    "input_tree_signal = TreeHandler('/home/shahid/cbmsoft/Data/PFSimplePlainTreeSignal.root','PlainTree')\n",
    "# We only select lambda candidates\n",
    "true_signal = input_tree_signal.get_subset('LambdaCandidates_is_signal == 1 & LambdaCandidates_mass>1.108 & LambdaCandidates_mass<1.1227')\n",
    "# Then we store the information in a dataframe object, sgnal, of Pandas library\n",
    "sgnal = true_signal.get_data_frame()\n",
    "\n",
    "# Similarly for the background\n",
    "input_tree_background = TreeHandler('/home/shahid/cbmsoft/Data/PFSimplePlainTreeBackground.root','PlainTree')\n",
    "true_background = input_tree_background.get_subset('LambdaCandidates_is_signal == 0 & 1.07< LambdaCandidates_mass < 1.108 or 1.1227 < LambdaCandidates_mass < 2.00')\n",
    "bg=true_background.get_data_frame()\n",
    "\n",
    "# The 10k events data set\n",
    "input_tree = TreeHandler('/home/shahid/cbmsoft/Data/10k_events_PFSimplePlainTree.root','PlainTree')\n",
    "df_original = input_tree.get_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_tree_signal\n",
    "del true_signal\n",
    "del input_tree_background\n",
    "del true_background\n",
    "del input_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The labels of the columns in the df data frame are having the prefix LambdaCandidates_ so we rename them\n",
    "new_labels= ['chi2geo', 'chi2primneg', 'chi2primpos', 'chi2topo', 'cosineneg',\n",
    "       'cosinepos', 'cosinetopo', 'distance', 'eta', 'l', 'ldl',\n",
    "       'mass', 'p', 'pT', 'phi', 'px', 'py', 'pz', 'rapidity',\n",
    "             'x', 'y', 'z', 'daughter1id', 'daughter2id', 'isfrompv', 'pid', 'issignal']\n",
    "\n",
    "df_original.columns=new_labels\n",
    "sgnal.columns = new_labels\n",
    "bg.columns = new_labels\n",
    "\n",
    "#Let's see how the dataframe object df looks like\n",
    "#df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above data frame object has some columns/features and for them at the very last column the true Monte Carlos information is available. This MC information tells us whether this reconstructed particle was originally produced as a decaying particle or not. So a value of 1 means that it is a true candidate and 0 means that it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Sometimes a data set contains entries which do not make sense. For example, infinite values or NaN entries. We clean the data by removing these entries. Ofcourse, we lose some data points but these outliers sometimes cause problems when we perform analysis. \n",
    "\n",
    "Since our experiment is a fixed target experiment so there are certain constraints which have to be applied on the data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new data frame and saving the results in it after cleaning of the original dfs\n",
    "#Also keeping the original one\n",
    "bcknd = clean_df(bg)\n",
    "signal = clean_df(sgnal)\n",
    "df_clean = clean_df(df_original)\n",
    "\n",
    "del bg\n",
    "del sgnal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_signal_mask = df_clean['issignal']>0\n",
    "df_clean_signal = df_clean[df_clean_signal_mask]\n",
    "df_clean_bac_mask = df_clean['issignal']==0\n",
    "df_clean_bac = df_clean[df_clean_bac_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_labels = ['background', 'signal']\n",
    "vars_to_draw = df_clean.columns\n",
    "\n",
    "ax = plot_utils.plot_distr([df_clean_bac, df_clean_signal], vars_to_draw, bins=100, labels=leg_labels, log=True, density=True, figsize=(30, 20), alpha=0.3, grid=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_signal.columns[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=27\n",
    "fig, axs = plt.subplots(figsize=(20, 10))\n",
    "df_clean_signal.iloc[:,l].plot.hist(bins = 1000,   alpha=0.2,facecolor='red', grid=False)\n",
    "df_clean_bac.iloc[:,l].plot.hist(bins = 1000,  alpha=0.2,facecolor='blue',grid=False)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(df_clean_signal.columns[l] + ' MC', fontsize = 15)\n",
    "plt.xlabel(df_clean_signal.columns[l])\n",
    "plt.legend(('signal','background'), fontsize = 15)\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['distance'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa = df_clean['distance']+0.001\n",
    "aa.plot.hist(bins=300, range=(0,0.1))\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(loc=0, scale=0.115810, size=(120093,1))\n",
    "new_series = pd.DataFrame(a)\n",
    "new_series.columns=['gauss']\n",
    "new_series1 = (0.11+asas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask12= (signal['distance']>=1)  & (signal['distance']<3)\n",
    "asas1=signal['distance'][mask12]\n",
    "asas1.describe()\n",
    "asas1.plot.hist(bins=300)\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uniform_random_array = np.random.uniform(low=0.000011, high=11.969676, size=(120870,1))\n",
    "uniform_random_df = pd.DataFrame(uniform_random_array)\n",
    "uniform_random_df.plot.hist(bins=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Background and Signal\n",
    "Our sample contains a lot of background (2178718) and somewhat signal candidates (36203). For analysis we will use a signal set of 4000 candidates and a background set of 12000 candidates. The background and signal candidates will be selected by using MC information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We randomly choose our signal set of 4000 candidates\n",
    "signal_selected= signal.sample(n=90000)\n",
    "\n",
    "#background = 3 times the signal is also done randomly\n",
    "background_selected = bcknd\n",
    "\n",
    "del signal\n",
    "del bcknd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's combine signal and background\n",
    "dfs = [signal_selected, background_selected]\n",
    "df_scaled = pd.concat(dfs)\n",
    "\n",
    "#del signal_selected\n",
    "#del background_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's shuffle the rows randomly\n",
    "df_scaled = df_scaled.sample(frac=1)\n",
    "del dfs\n",
    "# Let's take a look at the top 10 entries of the df\n",
    "df_scaled.iloc[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range1 = (1.075, 1.18)\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "#df_scaled['mass'].plot.hist(bins = 300, range=range1,grid=True,sharey=True)\n",
    "background_selected['mass'].plot.hist(bins = 300, facecolor='yellow',grid=True,range=range1)\n",
    "signal_selected['mass'].plot.hist(bins = 300, facecolor='magenta',grid=True, range=range1)\n",
    "plt.ylabel(\"Counts\", fontsize=15)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize= 15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title('Test and Train Lambda Invariant Mass', fontsize = 15)\n",
    "plt.legend(('Background', 'Signal'), fontsize = 15)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"signal_bac_invmass_MC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Test sets\n",
    "To make machine learning algorithms more efficient on unseen data we divide our data into two sets. One set is for training the algorithm and the other is for testing the algorithm. If we don't do this then the algorithm can overfit and we will not capture the general trends in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'chi2primneg', 'chi2primpos', 'chi2topo', 'cosinetopo', 'distance', 'ldl', \n",
    "         'x','y',\n",
    "       'daughter2id']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "cuts = [ 'chi2primneg', 'chi2primpos', 'ldl', 'distance', 'chi2geo','cosinepos']\n",
    "\n",
    "\n",
    "x = df_scaled[cuts].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MC information is saved in this y variable\n",
    "y =pd.DataFrame(df_scaled['issignal'], dtype='int')\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following columns will be used to predict whether a reconstructed candidate is a lambda particle or not\n",
    "x_whole = df_clean[cuts].copy()\n",
    "# The MC information is saved in this y variable\n",
    "y_whole = pd.DataFrame(df_clean['issignal'], dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_check_set= df_clean.copy()\n",
    "new_check_set['new_signal']=0\n",
    "mask1 = (new_check_set['chi2primpos'] > 18.4) & (new_check_set['chi2primneg'] > 18.4)\n",
    "\n",
    "mask2 = (new_check_set['ldl'] > 5) & (new_check_set['distance'] < 1)\n",
    "\n",
    "mask3 = (new_check_set['chi2geo'] < 3) & (new_check_set['cosinepos'] > 0) & (new_check_set['cosineneg'] > 0)\n",
    "\n",
    "new_check_set = new_check_set[(mask1) & (mask2) & (mask3)] \n",
    "\n",
    "#After all these cuts, what is left is considered as signal, so we replace all the values in the 'new_signal'\n",
    "# column by 1\n",
    "new_check_set['new_signal'] = 1\n",
    "signal_in_signal_mask = new_check_set['issignal']==1\n",
    "signal_in_signal = new_check_set[signal_in_signal_mask]\n",
    "signal_in_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>XGB Boost \n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate an XGBRegressor with default hyperparameter settings\n",
    "xgbr = xgb.XGBRegressor(max_depth = 8, alpha=10,gamma =1)\n",
    "\n",
    "# and compute a baseline to beat with hyperparameter optimization \n",
    "baseline = cross_val_score(xgbr, x, y, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = { \"n_estimators\": range(113,120)}\n",
    "\n",
    "rs = RandomizedSearchCV(xgbr, param_distributions=param_dist, \n",
    "                        scoring='roc_auc', n_iter=7)\n",
    "\n",
    "\n",
    "a= rs.fit(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBRFRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost hyper-parameter tuning\n",
    "def hyperParameterTuning(x, y):\n",
    "    param_tuning = {\n",
    "        'learning_rate': [0.01, 0.02, 0.03,0.1],\n",
    "        'max_depth': [3, 5, 8, 10,12,14],\n",
    "        'n_estimators' : [100, 200,300,400, 500],\n",
    "        'objective': ['reg:squarederror']\n",
    "    }\n",
    "\n",
    "    xgb_model = XGBRegressor()\n",
    "\n",
    "    gsearch = GridSearchCV(estimator = xgb_model,\n",
    "                           param_grid = param_tuning,                        \n",
    "                           scoring = 'roc_auc', #MAE\n",
    "                           #scoring = 'neg_mean_squared_error',  #MSE\n",
    "                           cv = 3,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 1)\n",
    "\n",
    "    gsearch.fit(x,y)\n",
    "\n",
    "    return gsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperParameterTuning(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(\n",
    "        objective = 'reg:squarederror',\n",
    "        learning_rate = 0.1,\n",
    "        max_depth = 8,\n",
    "        n_estimators = 200)\n",
    "\n",
    "%time xgb_model.fit(x_train, y_train, early_stopping_rounds=5, eval_set=[(x_test, y_test)], verbose=False)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(x_test)\n",
    "\n",
    "mae_xgb = roc_auc_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"MAE: \", mae_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian\n",
    "In order to find the best parameters of XGB for our data we use Bayesian optimization. Grid search and and random search could also do the same job but bayesian is more time efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "sum_array_numba = jit()(sum_array)\n",
    "jitted = %timeit -o sum_array_numba(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train, label = y_train)\n",
    "dtest = xgb.DMatrix(x_whole, label = y_whole)\n",
    "dtest1=xgb.DMatrix(x_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters\n",
    "\n",
    "*subsample* [default=1]\n",
    "Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. Subsampling will occur once in every boosting iteration.\n",
    "range: (0,1]\n",
    "\n",
    "*eta* [default=0.3, alias: learning_rate]\n",
    "Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "range: [0,1]\n",
    "\n",
    "\n",
    "*gamma* [default=0, alias: min_split_loss]\n",
    "Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "range: [0,∞]\n",
    "\n",
    "\n",
    "*alpha* [default=0, alias: reg_alpha]\n",
    "L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "*Lasso Regression* (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Optimization function for xgboost\n",
    "#specify the parameters you want to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, alpha, n_estimators ,learning_rate):\n",
    "    params = {'max_depth': int(max_depth),\n",
    "              'gamma': gamma,\n",
    "              'alpha':alpha,\n",
    "              'n_estimators': n_estimators,\n",
    "              'learning_rate':learning_rate,\n",
    "              'subsample': 0.8,\n",
    "              'eta': 0.1,\n",
    "              'eval_metric': 'auc', 'nthread' : 7}\n",
    "    cv_result = xgb.cv(params, dtrain, num_boost_round=70, nfold=5)\n",
    "    return  cv_result['test-auc-mean'].iloc[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (4, 10),\n",
    "                                             'gamma': (0, 1),\n",
    "                                            'alpha': (2,20),\n",
    "                                             'learning_rate':(0,1),\n",
    "                                             'n_estimators':(100,500)\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "xgb_bo.maximize(n_iter=15, init_points=8, acq='ei')\n",
    "#0.9952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best target so far 0.994872\n",
    "print(xgb_bo.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New models for treelite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_param = xgb_bo.max['params']\n",
    "param= {'alpha': max_param['alpha'], 'gamma': max_param['gamma'], 'learning_rate': max_param['learning_rate'], 'max_depth': int(round(max_param['max_depth'],0)), 'n_estimators': int(round(max_param['n_estimators'],0)), 'objective': 'binary:logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst1= bst.predict(dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_test = bst.predict(dtest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance(y_true, y_predict, y_true1, y_predict1):\n",
    "    roc_auc=roc_auc_score(y_true, y_predict)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    S0 = S0[~np.isnan(S0)]\n",
    "    xi = argmax(S0)\n",
    "    S0_best_threshold = (thresholds[xi])\n",
    "    \n",
    "    roc_auc1=roc_auc_score(y_true1, y_predict1)\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_true1, y_predict1,drop_intermediate=False ,pos_label=1)\n",
    "    S01 = sqrt(2 * ((tpr1 + fpr1) * log((1 + tpr1/fpr1)) - tpr1))\n",
    "    S01 = S01[~np.isnan(S01)]\n",
    "    xi1 = argmax(S01)\n",
    "    S0_best_threshold1 = (thresholds[xi1])\n",
    "    \n",
    "    fig, axs = plt.subplots(figsize=(15, 10), dpi = 100)\n",
    "    plt.plot(fpr, tpr, linestyle=':',color='darkorange',label='ROC curve train (area = %0.4f)' % roc_auc)\n",
    "    plt.plot(fpr1, tpr1, color='green',label='ROC curve test (area = %0.4f)' % roc_auc1)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold train set = '+\"%.4f\" % S0_best_threshold +'\\n S0 = '+ \"%.2f\" % S0[xi])\n",
    "    plt.scatter(fpr1[xi1], tpr1[xi1], marker='o', color='blue', label= 'Best Threshold test set = '+\"%.4f\" % S0_best_threshold1 +'\\n S0 = '+ \"%.2f\" % S01[xi1])\n",
    "    plt.xlabel('False Positive Rate', fontsize = 15)\n",
    "    plt.ylabel('True Positive Rate', fontsize = 15)\n",
    "    plt.legend(loc=\"lower right\", fontsize = 15)\n",
    "    plt.title('Receiver operating characteristic', fontsize = 15)\n",
    "    plt.xlim([-0.01, 1.0])\n",
    "    plt.ylim([0, 1.02])\n",
    "    #axs.axis([-0.01, 1, 0.9, 1])\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance(y_train, bst1,y_test, bst_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'] = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "df_clean['xgb_preds'].plot.hist(ax=ax, bins=300,facecolor='red',alpha = 0.3, bottom=0.001)\n",
    "TP = df_clean[(df_clean['issignal']==1)]\n",
    "TP['xgb_preds'].plot.hist(ax=ax, bins=300,facecolor='blue',alpha = 0.3, bottom=0.001)\n",
    "ax.set_yscale('log')\n",
    "plt.legend(('XGB predicted probability distribution','True positives = (MC =1) True lamdas in the distribution'), fontsize = 15, loc='upper right')\n",
    "plt.title(\"XGB returned probablility for the 10k events data set\", fontsize = 15)\n",
    "plt.ylabel(\"Log scale number of counts\", fontsize = 15)\n",
    "plt.xlabel(\"Probability\", fontsize = 15)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc=roc_auc_score(y_whole, df_clean['xgb_preds'])\n",
    "fpr, tpr, thresholds = roc_curve(y_whole, df_clean['xgb_preds'],drop_intermediate=False ,pos_label=1)\n",
    "S0 = sqrt(2 * ((tpr + fpr) * log(1 + tpr/fpr) - tpr))\n",
    "S0 = S0[~np.isnan(S0)]\n",
    "xi = argmax(S0)\n",
    "S0_best_threshold = (thresholds[xi])\n",
    "print('Best Threshold=%f, S0=%.3f' % (thresholds[xi], S0[xi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dtest.get_label()\n",
    "print('error=%f' %\n",
    "      (sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i]) /\n",
    "       float(len(preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite\n",
    "model = treelite.Model.from_xgboost(bst)\n",
    "toolchain = 'clang'\n",
    "model.export_lib(toolchain=toolchain, libpath='./mymodel.so',\n",
    "                 params={'parallel_comp': 8}, verbose=True)\n",
    "\n",
    "# Operating system of the target machine\n",
    "platform = 'unix'\n",
    "\n",
    "# Save the source package as a zip archive named mymodel.zip\n",
    "# Later, we'll use this package to produce the library mymodel.so.\n",
    "model.export_srcpkg(platform=platform, toolchain=toolchain,\n",
    "                    pkgpath='./mymodel.zip', libname='mymodel.so',\n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -l mymodel.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treelite_runtime     # runtime module\n",
    "predictor = treelite_runtime.Predictor('./mymodel.so', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_nump = pd.DataFrame.to_numpy(x_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = treelite_runtime.Batch.from_npy2d(x_whole_nump)\n",
    "df_clean['tree_lite'] = predictor.predict(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(out_pred,bins=300, range=(0.5,1) )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Already working model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(learning_rate = 0.09273,\n",
    "                max_depth = 9, alpha = 3.085, gamma =0.1481 , n_estimators = 115, nthread=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = xg_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xg_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'] =xg_reg.predict(x_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['xgb_preds'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC_score = roc_auc_score(y_train, xg_reg.predict(x_train))\n",
    "print(\"AUC: %f\" % (AUC_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_whole, df_clean['xgb_preds']))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_whole, df_clean['xgb_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(bst)\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.show()\n",
    "ax.figure.tight_layout() \n",
    "ax.figure.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_visualization(cut, range1=(1.09, 1.19), bins1= 300 ):\n",
    "    mask1 = df_clean['xgb_preds']>cut\n",
    "    df3=df_clean[mask1]\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "    color = 'tab:blue'\n",
    "    ax2.hist(df_clean['mass'],bins = bins1, range=range1, facecolor='blue',alpha = 0.35, label='before selection')\n",
    "    ax2.set_ylabel('Counts', fontsize = 15, color=color)\n",
    "    ax2.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend( fontsize = 15, loc='upper left')\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.hist(df3['mass'], bins = bins1, range=range1, facecolor='red',alpha = 0.35, label='Machine learning (XGB)')\n",
    "    ax1.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax1.set_ylabel('Counts ', fontsize = 15, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend( fontsize = 15,loc='upper right' )\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.title(\"The sample's Invariant Mass with XGB (with a cut > \"+str(cut)+')', fontsize = 15)\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_visualization(S0_best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = df_clean['xgb_preds']>0.969618\n",
    "df3=df_clean[mask1]\n",
    "df3_clean_signal_mask = df3['issignal']>0\n",
    "df3_clean_signal = df3[df_clean_signal_mask]\n",
    "df3_clean_bac_mask = df3['issignal']==0\n",
    "df3_clean_bac = df3[df_clean_bac_mask]\n",
    "#vars_to_draw.remove('xgb_preds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask10 = (df_clean['xgb_preds1']==1)&(df_clean['issignal']==1)\n",
    "df4=df_clean[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leg_labels = ['original background', 'background inside XGB predicted signal']\n",
    "vars_to_draw = list(df3.columns)\n",
    "ax = plot_utils.plot_distr([df_clean_bac, df3_clean_bac], vars_to_draw, bins=100, labels=leg_labels, log=True, density=True, figsize=(30, 20), alpha=0.3, grid=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hits.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = S0_best_threshold\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3=df_clean[mask1]\n",
    "fig, axs = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "range1= (1.09, 1.7)\n",
    "\n",
    "#xgb\n",
    "\n",
    "df3_new = df3[df3['issignal']==1]\n",
    "df3_new1 = df3[df3['issignal']==0]\n",
    "df3['mass'].plot.hist(bins = 300, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True)\n",
    "#df3_new['mass'].plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True)\n",
    "df3_new1['mass'].plot.hist(bins = 300, range=range1,facecolor='green',alpha = 0.3,grid=True,sharey=True)\n",
    "plt.legend(('XGB selected lambdas','\\n False positives = \\n (MC =0)\\n background in \\n the distribution' ), fontsize = 18, loc='upper right')\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "plt.title(\"KFPF variables + cos$\\Theta_{between \\ \\overrightarrow{P_\\Lambda} \\  & \\ \\overrightarrow{P_{\\Pi^-}}}$ + $P_T$  with a cut of %.4f \"%cut3 +\"on the XGB probability distribution\", fontsize = 18)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 18)\n",
    "plt.ylabel(\"Counts\", fontsize = 18)\n",
    "axs.tick_params(labelsize=18)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "By definition a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.\n",
    "\n",
    "Thus in binary classification, the count of true positives is $C_{0,0}$, false positives is $C_{1,0}$, true negatives is $C_{1,1}$ and false negatives is $C_{0,1}$.\n",
    "\n",
    "The following function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label',fontsize = 15)\n",
    "    plt.xlabel('Predicted label',fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut1 = S0_best_threshold\n",
    "df_clean['xgb_preds1'] = ((df_clean['xgb_preds']>cut1)*1)\n",
    "cnf_matrix = confusion_matrix(y_whole, df_clean['xgb_preds1'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['signal','background'], title='Confusion Matrix for XGB for cut > '+str(cut1))\n",
    "plt.savefig('confusion_matrix_extreme_gradient_boosting_whole_data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pT_vs_rapidity(var_yaxis, var_xaxis, range_var_yaxis, range_var_xaxis):\n",
    "    import matplotlib as mpl\n",
    "    fig, axs = plt.subplots(figsize=(15, 10))\n",
    "    plt.hist2d(var_yaxis,var_xaxis,range=[range_var_yaxis,range_var_xaxis], bins=100, norm=mpl.colors.LogNorm(), cmap=plt.cm.Greys)\n",
    "    plt.xlabel('pT', fontsize=15)\n",
    "    plt.ylabel('rapidity', fontsize=15)\n",
    "    plt.show()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/pT_vs_rapidity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df_clean[df_clean['xgb_preds1']==1] \n",
    "range1=[-0.1, 3.5]\n",
    "range2=[-0.5, 3]\n",
    "\n",
    "pT_vs_rapidity(new_df['rapidity'], new_df['pT'], range1, range2)\n",
    "#fig.savefig(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/pT_vs_rapidity_xgb.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_new = new_df[new_df['issignal']==1]\n",
    "range1=[-0.1, 3.5]\n",
    "range2=[-0.5, 3]\n",
    "\n",
    "pT_vs_rapidity(df3_new['rapidity'], df3_new['pT'], range1, range2)\n",
    "\n",
    "#fig.savefig(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/pT_vs_rapidity_xgb_true.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df1=df_clean[df_clean['issignal']==1]\n",
    "range1=[-0.1, 3.5]\n",
    "range2=[-0.5, 3]\n",
    "\n",
    "pT_vs_rapidity(new_df1['rapidity'], new_df1['pT'], range1, range2)\n",
    "#fig.savefig(\"/home/shahid/cbmsoft/Cut_optimization/uncut_data/pT_vs_rapidity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-1.03)/(1+np.exp(-1.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_selected['cosineneg'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts1 =['pz','cosineneg']\n",
    "bac_new = background_selected[background_selected['mass']>1.1227]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bac_new['cosineneg'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_plot = sb.pairplot(bac_new,vars = cuts1, height=5)\n",
    "sb_plot.savefig(\"output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['kfpf_preds']=0\n",
    "df_clean['kfpf_preds'] = (df_clean['chi2primpos'] > 18.4)*1\n",
    "df_clean['kfpf_preds'] =(df_clean['chi2primneg'] > 18.4)*1\n",
    "df_clean['kfpf_preds'] = (df_clean['ldl'] > 5)*1\n",
    "df_clean['kfpf_preds'] = (df_clean['distance'] < 1)*1\n",
    "df_clean['kfpf_preds'] = (df_clean['chi2geo'] < 3)*1\n",
    "df_clean['kfpf_preds'] = (df_clean['cosinepos'] > 0)*1\n",
    "df_clean['kfpf_preds'] = (df_clean['cosineneg'] > 0)*1\n",
    "df_clean['kfpf_preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnf_matrix = confusion_matrix(new_check_set['issignal'], new_check_set['new_signal'], labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "fig, axs = plt.subplots(figsize=(10, 8))\n",
    "axs.yaxis.set_label_coords(-0.04,.5)\n",
    "axs.xaxis.set_label_coords(0.5,-.005)\n",
    "plot_confusion_matrix(cnf_matrix, classes=['signal','background'], title='Confusion Matrix for XGB for cut > '+str(cut1))\n",
    "plt.savefig('confusion_matrix_extreme_gradient_boosting_whole_data.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10725/np.sqrt(30746)\n",
    "\n",
    "\n",
    "#[[   10755     2154]\n",
    " #[   28054 20686209]]\n",
    "10755/np.sqrt(20686209+10755)\n",
    "\n",
    "#[[   10571     2338]\n",
    "# [   26070 20688193]]\n",
    "10571/np.sqrt(20688193+10571)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc=roc_auc_score(y_whole, df_clean['xgb_preds'])\n",
    "fpr, tpr, thresholds = roc_curve(y_whole, df_clean['xgb_preds'],drop_intermediate=False ,pos_label=1)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "S0 = S0[~np.isnan(S0)]\n",
    "xi = argmax(S0)\n",
    "S0_best_threshold = (thresholds[xi])\n",
    "print('Best Threshold=%f, S0=%.3f' % (thresholds[xi], S0[xi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# calculate the geometric-mean for each threshold\n",
    "gmeans = sqrt(tpr * (1-fpr))\n",
    "# locate the index of the largest g-mean\n",
    "ix = argmax(gmeans)\n",
    "a = str(thresholds[ix])\n",
    "print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significance(y_true, y_predict):\n",
    "    roc_auc=roc_auc_score(y_true, y_predict)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_predict,drop_intermediate=False ,pos_label=1)\n",
    "    S0 = sqrt(2 * ((tpr + fpr) * log((1 + tpr/fpr)) - tpr))\n",
    "    S0 = S0[~np.isnan(S0)]\n",
    "    xi = argmax(S0)\n",
    "    S0_best_threshold = (thresholds[xi])\n",
    "    print('Best Threshold=%f, S0=%.3f' % (thresholds[xi], S0[xi]))\n",
    "    print('AUC score : ', roc_auc)\n",
    "    return fpr, tpr, S0_best_threshold, S0[xi], xi\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance(y_whole, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc=roc_auc_score(y_whole, preds)\n",
    "fpr, tpr, thresholds = roc_curve(y_whole, preds,drop_intermediate=False ,pos_label=1)\n",
    "S0 = sqrt(2 * ((tpr + fpr) * log(1 + tpr/fpr) - tpr))\n",
    "S0 = S0[~np.isnan(S0)]\n",
    "xi = argmax(S0)\n",
    "S0_best_threshold = (thresholds[xi])\n",
    "print('Best Threshold=%f, S0=%.3f' % (thresholds[xi], S0[xi]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(15, 10), dpi = 100)\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "          label='ROC curve (area = %0.4f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.scatter(fpr[xi], tpr[xi], marker='o', color='black', label= 'Best Threshold = '+\"%.4f\" % S0_best_threshold +'\\n S0 = '+ \"%.2f\" % S0[xi])\n",
    "plt.xlabel('False Positive Rate', fontsize = 15)\n",
    "plt.ylabel('True Positive Rate', fontsize = 15)\n",
    "plt.legend(loc=\"lower right\", fontsize = 15)\n",
    "plt.title('Receiver operating characteristic', fontsize = 15)\n",
    "plt.xlim([-0.01, 1.0])\n",
    "plt.ylim([0, 1.02])\n",
    "#axs.axis([-0.01, 1, 0.9, 1])\n",
    "fig.tight_layout()\n",
    "plt.savefig('ROC_and_AUC.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:3em;color:purple; font-style:bold\"><br>Tree visualization\n",
    "<br></p><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax1 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "xgb.plot_tree(xg_reg,num_trees=10)\n",
    "plt.rcParams['figure.figsize'] = [20, 40]\n",
    "plt.rcParams['figure.dpi']=200\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.to_graphviz(xg_reg, fmap='', num_trees=0, rankdir=None, yes_color=None, no_color=None, condition_node_params=None, leaf_node_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define objective function\n",
    "def f(x):\n",
    "    return {'loss': x ** 2 - x, 'status': STATUS_OK}\n",
    "\n",
    "# Run hyperopt optimization\n",
    "trials = Trials()\n",
    "result = fmin(\n",
    "    fn=f,                           # objective function\n",
    "    space=hp.uniform('x', -1, 1),   # parameter space\n",
    "    algo=tpe.suggest,               # surrogate algorithm\n",
    "    max_evals=500,                  # no. of evaluations\n",
    "    trials=trials                   # trials object that keeps track of the sample results (optional)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Print the optimized parameters\n",
    "print(result)   # {'x': 0.5000833960783931}\n",
    "\n",
    "# Extract and plot the trials \n",
    "x = trials.vals['x']\n",
    "y = [x['loss'] for x in trials.results]\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# XGB parameters\n",
    "xgb_reg_params = {\n",
    "    'learning_rate':    hp.choice('learning_rate',    np.arange(0.05, 0.31, 0.05)),\n",
    "    'max_depth':        hp.choice('max_depth',        np.arange(5, 16, 1, dtype=int)),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 8, 1, dtype=int)),\n",
    "    'colsample_bytree': hp.choice('colsample_bytree', np.arange(0.3, 0.8, 0.1)),\n",
    "    'subsample':        hp.uniform('subsample', 0.8, 1),\n",
    "    'n_estimators':     100,\n",
    "}\n",
    "xgb_fit_params = {\n",
    "    'eval_metric': 'rmse',\n",
    "    'early_stopping_rounds': 10,\n",
    "    'verbose': False\n",
    "}\n",
    "xgb_para = dict()\n",
    "xgb_para['reg_params'] = xgb_reg_params\n",
    "xgb_para['fit_params'] = xgb_fit_params\n",
    "xgb_para['loss_func' ] = lambda y, pred: np.sqrt(mean_squared_error(y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials\n",
    "\n",
    "\n",
    "class HPOpt(object):\n",
    "\n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.x_test  = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test  = y_test\n",
    "\n",
    "    def process(self, fn_name, space, trials, algo, max_evals):\n",
    "        fn = getattr(self, fn_name)\n",
    "        try:\n",
    "            result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
    "        except Exception as e:\n",
    "            return {'status': STATUS_FAIL,\n",
    "                    'exception': str(e)}\n",
    "        return result, trials\n",
    "\n",
    "    def xgb_reg(self, para):\n",
    "        reg = xgb.XGBRegressor(**para['reg_params'])\n",
    "        return self.train_reg(reg, para)\n",
    "\n",
    "    def train_reg(self, reg, para):\n",
    "        reg.fit(self.x_train, self.y_train,\n",
    "                eval_set=[(self.x_train, self.y_train), (self.x_test, self.y_test)],\n",
    "                **para['fit_params'])\n",
    "        pred = reg.predict(self.x_test)\n",
    "        loss = para['loss_func'](self.y_test, pred)\n",
    "        return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = HPOpt(x_train, x_test, y_train, y_test)\n",
    "\n",
    "xgb_opt = obj.process(fn_name='xgb_reg', space=xgb_para, trials=Trials(), algo=tpe.suggest, max_evals=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clf = xgb.XGBClassifier()\n",
    "model_hdl = ModelHandler(model_clf, cuts)\n",
    "train_test_data = train_test_generator([y, x], [1,0], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_pars_ranges = {'n_estimators': (200, 1000), 'max_depth': (2, 4), 'learning_rate': (0.01, 0.1)}\n",
    "model_hdl.optimize_params_bayes(df_, hyper_pars_ranges, 'roc_auc', nfold=5, init_points=5, n_iter=5, njobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Matplotlib Animation Example\n",
    "\n",
    "author: Jake Vanderplas\n",
    "email: vanderplas@astro.washington.edu\n",
    "website: http://jakevdp.github.com\n",
    "license: BSD\n",
    "Please feel free to use and modify this, but keep the above information. Thanks!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(0, 2), ylim=(-2, 2))\n",
    "line, = ax.plot([], [], lw=2)\n",
    "\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return line,\n",
    "\n",
    "# animation function.  This is called sequentially\n",
    "def animate(i):\n",
    "    x = np.linspace(0, 2, 1000)\n",
    "    y = np.sin(2 * np.pi * (x - 0.01 * i))\n",
    "    line.set_data(x, y)\n",
    "    return line,\n",
    "\n",
    "# call the animator.  blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=200, interval=20, blit=True)\n",
    "\n",
    "# save the animation as an mp4.  This requires ffmpeg or mencoder to be\n",
    "# installed.  The extra_args ensure that the x264 codec is used, so that\n",
    "# the video can be embedded in html5.  You may need to adjust this for\n",
    "# your system: for more information, see\n",
    "# http://matplotlib.sourceforge.net/api/animation_api.html\n",
    "anim.save('basic_animation.mp4', fps=30, extra_args=['-vcodec', 'libx264'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "def f(x, y):\n",
    "    return np.sin(x) + np.cos(y)\n",
    "\n",
    "x = np.linspace(0, 2 * np.pi, 120)\n",
    "y = np.linspace(0, 2 * np.pi, 100).reshape(-1, 1)\n",
    "\n",
    "# ims is a list of lists, each row is a list of artists to draw in the\n",
    "# current frame; here we are just animating one artist, the image, in\n",
    "# each frame\n",
    "ims = []\n",
    "for i in range(60):\n",
    "    x += np.pi / 15.\n",
    "    y += np.pi / 20.\n",
    "    im = ax.imshow(f(x, y), animated=True)\n",
    "    if i == 0:\n",
    "        ax.imshow(f(x, y))  # show an initial one first\n",
    "    ims.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "\n",
    "# To save the animation, use e.g.\n",
    "#\n",
    "# ani.save(\"movie.mp4\")\n",
    "#\n",
    "# or\n",
    "#\n",
    "writer = animation.FFMpegWriter(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "ani.save(\"movie.mp4\", writer=writer)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.path as path\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "# histogram our data with numpy\n",
    "data = np.random.randn(1000)\n",
    "n, bins = np.histogram(data, 100)\n",
    "\n",
    "# get the corners of the rectangles for the histogram\n",
    "left = bins[:-1]\n",
    "right = bins[1:]\n",
    "bottom = np.zeros(len(left))\n",
    "top = bottom + n\n",
    "nrects = len(left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nverts = nrects * (1 + 3 + 1)\n",
    "verts = np.zeros((nverts, 2))\n",
    "codes = np.full(nverts, path.Path.LINETO)\n",
    "codes[0::5] = path.Path.MOVETO\n",
    "codes[4::5] = path.Path.CLOSEPOLY\n",
    "verts[0::5, 0] = left\n",
    "verts[0::5, 1] = bottom\n",
    "verts[1::5, 0] = left\n",
    "verts[1::5, 1] = top\n",
    "verts[2::5, 0] = right\n",
    "verts[2::5, 1] = top\n",
    "verts[3::5, 0] = right\n",
    "verts[3::5, 1] = bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = None\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    # simulate new data coming in\n",
    "    data = np.random.randn(1000)\n",
    "    n, bins = np.histogram(data, 100)\n",
    "    top = bottom + n\n",
    "    verts[1::5, 1] = top\n",
    "    verts[2::5, 1] = top\n",
    "    return [patch, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "barpath = path.Path(verts, codes)\n",
    "patch = patches.PathPatch(\n",
    "    barpath, facecolor='green', edgecolor='yellow', alpha=0.5)\n",
    "ax.add_patch(patch)\n",
    "\n",
    "ax.set_xlim(left[0], right[-1])\n",
    "ax.set_ylim(bottom.min(), top.max())\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, 50, repeat=False, blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from celluloid import Camera\n",
    "\n",
    "fig, axes = plt.subplots(2)\n",
    "camera = Camera(fig)\n",
    "t = np.linspace(0, 2 * np.pi, 128, endpoint=False)\n",
    "for i in t:\n",
    "    axes[0].plot(t, np.sin(t + i), color='blue')\n",
    "    axes[1].plot(t, np.sin(t - i), color='blue')\n",
    "    camera.snap()\n",
    "    \n",
    "animation = camera.animate()  \n",
    "animation.save('celluloid_subplots.gif', writer = 'imagemagick')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bns = 1000\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15,10))\n",
    "ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7 = axes.flatten()\n",
    "\n",
    "ax0.hist(true_signal.iloc[:,0], bins=n_bns, alpha=0.2, facecolor='red', stacked=True, log= True,label='Signal')\n",
    "ax0.hist(true_bac.iloc[:,0], bins=n_bns, alpha=0.2, facecolor='blue', stacked=True, log= True,label='Background')\n",
    "ax0.legend(prop={'size': 10})\n",
    "ax0.set_title('chi2 geometric')\n",
    "\n",
    "\n",
    "ax1.hist(true_signal.iloc[:,1], bins=n_bns, alpha=0.2, facecolor='red',range=(100,1000000),stacked=True, log= True,label='Signal')\n",
    "ax1.hist(true_bac.iloc[:,1], bins=n_bns, alpha=0.2, facecolor='blue',range=(100,1000000),stacked=True, log= True,label='Background')\n",
    "ax1.set_title('chi2 primary pion')\n",
    "ax1.legend(prop={'size': 10})\n",
    "\n",
    "ax2.hist(true_signal.iloc[:,2], bins=n_bns, alpha=0.2, facecolor='red',range=(100,100000),stacked=True, log= True,label='Signal')\n",
    "ax2.hist(true_bac.iloc[:,2], bins=n_bns, alpha=0.2, facecolor='blue',range=(100,100000),stacked=True, log= True,label='Background')\n",
    "ax2.set_title('chi2 primary proton')\n",
    "ax2.legend(prop={'size': 10})\n",
    "\n",
    "ax3.hist(true_signal.iloc[:,3], bins=n_bns, alpha=0.2, facecolor='red',range=(-1,30),stacked=True, log= True,label='Signal')\n",
    "ax3.hist(true_bac.iloc[:,3], bins=n_bns, alpha=0.2, facecolor='blue',range=(-1,30),stacked=True, log= True,label='Background')\n",
    "ax3.set_title('chi2 topological')\n",
    "ax3.legend(prop={'size': 10})\n",
    "\n",
    "\n",
    "ax4.hist(true_signal.iloc[:,6], bins=n_bns, alpha=0.2, facecolor='red',range=(0,5000),stacked=True, log= True,label='Signal')\n",
    "ax4.hist(true_bac.iloc[:,6], bins=n_bns, alpha=0.2, facecolor='blue',range=(0,5000),stacked=True, log= True,label='Background')\n",
    "ax4.set_title('l/dl')\n",
    "ax4.legend(prop={'size': 10})\n",
    "\n",
    "ax5.hist(true_signal.iloc[:,14], bins=n_bns, alpha=0.2, facecolor='red', stacked=True, log= True,label='Signal')\n",
    "ax5.hist(true_bac.iloc[:,14], bins=n_bns, alpha=0.2, facecolor='blue', stacked=True, log= True,label='Background')\n",
    "ax5.set_title('Distance between secondary tracks (corr= 0.178)')\n",
    "ax5.legend(prop={'size': 10})\n",
    "\n",
    "ax6.hist(true_signal.iloc[:,17], bins=n_bns, alpha=0.2, facecolor='red', stacked=True, log= True,label='Signal')\n",
    "ax6.hist(true_bac.iloc[:,17], bins=n_bns, alpha=0.2, facecolor='blue', stacked=True, log= True,label='Background')\n",
    "ax6.set_title('cosine of angle between proton and lambda (corr= -0.193)')\n",
    "ax6.legend(prop={'size': 10})\n",
    "\n",
    "ax7.hist(true_signal.iloc[:,16], bins=n_bns, alpha=0.2, facecolor='red',range=(0.65,1),stacked=True, log= True,label='Signal')\n",
    "ax7.hist(true_bac.iloc[:,16], bins=n_bns, alpha=0.2, facecolor='blue',range=(0.65,1),stacked=True, log= True,label='Background')\n",
    "ax7.set_title('cosine of angle between pion and lambda (corr=-0.25)')\n",
    "ax7.legend(prop={'size': 10})\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"KFPF_chi2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terminal      conda activate myrootenv\n",
    "#terminal      root --notebook\n",
    "import ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = ROOT.TH1F(\"myHist\", \"myTitle\", 64, -4, 4)\n",
    "h.FillRandom(\"gaus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ROOT.TF1(\"f1\", \"sin(x)/x\", 0., 10.)\n",
    "f.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will display the inavriant mass histogram of the original 10k event set along with the mass histoigram after we apply a cut\n",
    "# on the probability prediction of xgb\n",
    "def cut_visualization(cut, range1=(1.09, 1.19), bins1= 300 ):\n",
    "    mask1 = df_clean['xgb_preds']>cut\n",
    "    df3=df_clean[mask1]\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=(15, 10), dpi = 200)\n",
    "    color = 'tab:blue'\n",
    "    ax2.hist(new_check_set['mass'],bins = bins1, range=range1, facecolor='blue',alpha = 0.35, label='KFPF')\n",
    "    ax2.set_ylabel('Counts', fontsize = 15, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    ax2.legend( fontsize = 15, loc='upper left')\n",
    "    \n",
    "    color = 'tab:red'\n",
    "    ax1 = ax2.twinx()\n",
    "    ax1.hist(df3['mass'], bins = bins1, range=range1, facecolor='red',alpha = 0.35, label='XGB')\n",
    "    ax1.set_xlabel('Mass in GeV', fontsize = 15)\n",
    "    ax1.set_ylabel('Counts ', fontsize = 15, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend( fontsize = 15,loc='upper right' )\n",
    "\n",
    "    plt.title(\"The original sample's Invariant Mass along with mass after selection of XGB (with a cut > \"+str(cut)+')', fontsize = 15)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"hists.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_visualization(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_check_set= df_clean.copy()\n",
    "new_check_set['new_signal']=0\n",
    "mask1 = (new_check_set['chi2primpos'] > 18.4) & (new_check_set['chi2primneg'] > 18.4)\n",
    "\n",
    "mask2 = (new_check_set['ldl'] > 5) & (new_check_set['distance'] < 1)\n",
    "\n",
    "mask3 = (new_check_set['chi2geo'] < 3) & (new_check_set['cosinepos'] > 0) & (new_check_set['cosineneg'] > 0)\n",
    "\n",
    "new_check_set = new_check_set[(mask1) & (mask2) & (mask3)] \n",
    "\n",
    "#After all these cuts, what is left is considered as signal, so we replace all the values in the 'new_signal'\n",
    "# column by 1\n",
    "new_check_set['new_signal'] = 1\n",
    "new_check_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = S0_best_threshold\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3=df_clean[mask1]\n",
    "fig, axs = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "range1= (1.09, 1.17)\n",
    "\n",
    "#xgb\n",
    "\n",
    "\n",
    "(df3['mass']).plot.hist(bins = 300, range=range1, facecolor='red',alpha = 0.3,grid=True,sharey=True)\n",
    "(new_check_set['mass']).plot.hist(bins = 300, range=range1,facecolor='blue',alpha = 0.3,grid=True,sharey=True)\n",
    "plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "plt.ylabel(\"counts\", fontsize = 15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.legend(('XGBoost','KFPF'), fontsize = 15, loc='upper left')\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "plt.title(\"The lambda's Invariant Mass histogram with KFPF and XGB selection criteria\", fontsize = 15)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (1.09, 1.17)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3_base['mass']),bins = 300, range=range1, facecolor='red',alpha = 0.3)\n",
    "ns1, bins1, patches1=axs[0].hist((new_check_set['mass']),bins = 300, range=range1,facecolor='blue',alpha = 0.3)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 15)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGBoost Selected $\\Lambda$s','KFPF selected $\\Lambda$s'), fontsize = 15, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The lambda's Invariant Mass histogram with KFPF and XGB selection criteria on KFPF variables\", fontsize = 15)\n",
    "axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3_base['mass'],range=(1.09, 1.17), bins=300)\n",
    "hist2, bin_edges2 = np.histogram(new_check_set['mass'],range=(1.09, 1.17), bins=300)\n",
    "\n",
    "#makes sense to have only positive values \n",
    "diff = (hist1 - hist2)\n",
    "axs[1].bar(bins[:-1],     # this is what makes it comparable\n",
    "        ns / ns1, # maybe check for div-by-zero!\n",
    "        width=0.001)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 15)\n",
    "axs[1].set_ylabel(\"XGB / KFPF\", fontsize = 15)\n",
    "axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = S0_best_threshold\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3_base_cospos=df_clean[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut3 = S0_best_threshold\n",
    "mask1 = df_clean['xgb_preds']>cut3\n",
    "df3_base=df_clean[mask1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "\n",
    "range1= (1.09, 1.17)\n",
    "bin1 = 200\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1,figsize=(15,10), sharex=True,  gridspec_kw={'width_ratios': [10],\n",
    "                           'height_ratios': [8,4]})\n",
    "\n",
    "ns, bins, patches=axs[0].hist((df3_base_cospos['mass']),bins = bin1, range=range1, facecolor='red',alpha = 0.3)\n",
    "ns1, bins1, patches1=axs[0].hist((df3_base['mass']),bins = bin1, range=range1,facecolor='blue',alpha = 0.3)\n",
    "#plt.xlabel(\"Mass in GeV\", fontsize = 15)\n",
    "axs[0].set_ylabel(\"counts\", fontsize = 15)\n",
    "#axs[0].grid()\n",
    "axs[0].legend(('XGB base+ cos$\\Theta_{between \\ \\overrightarrow{P_\\Lambda} \\  & \\ \\overrightarrow{P_{proton}}}$ Selected $\\Lambda$s','XGB base selected $\\Lambda$s'), fontsize = 15, loc='upper right')\n",
    "\n",
    "#plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "axs[0].set_title(\"The lambda's Invariant Mass histogram with XGB base and XGB base+ cos$\\Theta_{between \\ \\overrightarrow{P_\\Lambda} \\  & \\ \\overrightarrow{P_{proton}}}$ selection criteria\", fontsize = 15)\n",
    "axs[0].grid()\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=15)\n",
    "#fig.savefig(\"whole_sample_invmass_with_ML.png\")\n",
    "\n",
    "\n",
    "hist1, bin_edges1 = np.histogram(df3_base_cospos['mass'],range=range1, bins=bin1)\n",
    "hist2, bin_edges2 = np.histogram(df3_base['mass'],range=range1, bins=bin1)\n",
    "\n",
    "#makes sense to have only positive values \n",
    "diff = (hist1 - hist2)\n",
    "axs[1].bar(bins[:-1],     # this is what makes it comparable\n",
    "        ns / ns1, # maybe check for div-by-zero!\n",
    "        width=0.001)\n",
    "plt.xlabel(\"Mass in $\\dfrac{GeV}{c^2}$\", fontsize = 15)\n",
    "axs[1].set_ylabel(\"XGB base+cospos / XGB base\", fontsize = 15)\n",
    "axs[1].grid()\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.show()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"whole_sample_invmass_with_ML.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
